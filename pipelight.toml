# Pipelight CI/CD for 71-Shard Challenge Framework
# Runs jobs for all agent frameworks and evaluates results

[[pipelines]]
name = "test"
steps = [
  { name = "lint", commands = ["cargo clippy --all-targets"] },
  { name = "test", commands = ["cargo test --all"] },
]

[[pipelines]]
name = "generate-shards"
steps = [
  { name = "build", commands = ["cd shard0/recon && cargo build --release --bin generate-shards"] },
  { name = "generate", commands = ["cd shard0/recon && cargo run --release --bin generate-shards"] },
  { name = "verify", commands = ["test -f shard_challenges.json && test -f zk_proof_templates.json"] },
]

[[pipelines]]
name = "recon-tor"
steps = [
  { name = "check-tor", commands = ["systemctl is-active tor || sudo systemctl start tor"] },
  { name = "build", commands = ["cd shard0/recon && cargo build --release --bin recon"] },
  { name = "scan", commands = ["cd shard0/recon && cargo run --release --bin recon"] },
]

[[pipelines]]
name = "zktls-witness"
steps = [
  { name = "check-tor", commands = ["systemctl is-active tor || sudo systemctl start tor"] },
  { name = "build", commands = ["cd shard0/recon && cargo build --release --bin zktls"] },
  { name = "generate", commands = ["cd shard0/recon && cargo run --release --bin zktls"] },
  { name = "verify-parquet", commands = ["ls -lh shard*_zktls.parquet"] },
]

[[pipelines]]
name = "eval-claude"
steps = [
  { name = "setup", commands = ["pip install anthropic"] },
  { name = "run-level0", commands = ["python3 agents/claude_agent.py --level 0"] },
  { name = "verify-proof", commands = ["python3 agents/verify_proof.py --agent claude --level 0"] },
  { name = "score", commands = ["python3 agents/score.py --agent claude"] },
]

[[pipelines]]
name = "eval-openai"
steps = [
  { name = "setup", commands = ["pip install openai"] },
  { name = "run-level0", commands = ["python3 agents/openai_agent.py --level 0"] },
  { name = "verify-proof", commands = ["python3 agents/verify_proof.py --agent openai --level 0"] },
  { name = "score", commands = ["python3 agents/score.py --agent openai"] },
]

[[pipelines]]
name = "eval-ollama"
steps = [
  { name = "setup", commands = ["pip install ollama"] },
  { name = "run-level0", commands = ["python3 agents/ollama_agent.py --level 0"] },
  { name = "verify-proof", commands = ["python3 agents/verify_proof.py --agent ollama --level 0"] },
  { name = "score", commands = ["python3 agents/score.py --agent ollama"] },
]

[[pipelines]]
name = "eval-autogen"
steps = [
  { name = "setup", commands = ["pip install pyautogen"] },
  { name = "run-level0", commands = ["python3 agents/autogen_agent.py --level 0"] },
  { name = "verify-proof", commands = ["python3 agents/verify_proof.py --agent autogen --level 0"] },
  { name = "score", commands = ["python3 agents/score.py --agent autogen"] },
]

[[pipelines]]
name = "eval-langchain"
steps = [
  { name = "setup", commands = ["pip install langchain"] },
  { name = "run-level0", commands = ["python3 agents/langchain_agent.py --level 0"] },
  { name = "verify-proof", commands = ["python3 agents/verify_proof.py --agent langchain --level 0"] },
  { name = "score", commands = ["python3 agents/score.py --agent langchain"] },
]

[[pipelines]]
name = "eval-crewai"
steps = [
  { name = "setup", commands = ["pip install crewai"] },
  { name = "run-level0", commands = ["python3 agents/crewai_agent.py --level 0"] },
  { name = "verify-proof", commands = ["python3 agents/verify_proof.py --agent crewai --level 0"] },
  { name = "score", commands = ["python3 agents/score.py --agent crewai"] },
]

[[pipelines]]
name = "eval-all-agents"
steps = [
  { name = "claude", commands = ["pipelight run eval-claude"] },
  { name = "openai", commands = ["pipelight run eval-openai"] },
  { name = "ollama", commands = ["pipelight run eval-ollama"] },
  { name = "autogen", commands = ["pipelight run eval-autogen"] },
  { name = "langchain", commands = ["pipelight run eval-langchain"] },
  { name = "crewai", commands = ["pipelight run eval-crewai"] },
  { name = "leaderboard", commands = ["python3 agents/generate_leaderboard.py"] },
]

[[pipelines]]
name = "eval-shard"
steps = [
  { name = "parse-args", commands = ["echo 'Evaluating shard $SHARD_ID'"] },
  { name = "load-challenge", commands = ["python3 agents/load_shard.py --id $SHARD_ID"] },
  { name = "run-agents", commands = ["pipelight run eval-all-agents"] },
  { name = "aggregate", commands = ["python3 agents/aggregate_results.py --shard $SHARD_ID"] },
]

[[pipelines]]
name = "eval-all-shards"
steps = [
  { name = "crypto", commands = ["for i in {0..70}; do SHARD_ID=$i pipelight run eval-shard; done"] },
  { name = "encryption", commands = ["for i in {71..141}; do SHARD_ID=$i pipelight run eval-shard; done"] },
  { name = "prompt", commands = ["for i in {142..212}; do SHARD_ID=$i pipelight run eval-shard; done"] },
  { name = "multiagent", commands = ["for i in {213..283}; do SHARD_ID=$i pipelight run eval-shard; done"] },
  { name = "reversing", commands = ["for i in {284..354}; do SHARD_ID=$i pipelight run eval-shard; done"] },
  { name = "economic", commands = ["for i in {355..425}; do SHARD_ID=$i pipelight run eval-shard; done"] },
  { name = "meta", commands = ["for i in {426..496}; do SHARD_ID=$i pipelight run eval-shard; done"] },
  { name = "final-report", commands = ["python3 agents/final_report.py"] },
]

[[pipelines]]
name = "benchmark"
steps = [
  { name = "generate", commands = ["pipelight run generate-shards"] },
  { name = "eval", commands = ["pipelight run eval-all-agents"] },
  { name = "report", commands = ["python3 agents/benchmark_report.py"] },
]

[[pipelines]]
name = "full"
steps = [
  { name = "test", commands = ["pipelight run test"] },
  { name = "generate", commands = ["pipelight run generate-shards"] },
  { name = "recon", commands = ["pipelight run recon-tor"] },
  { name = "zktls", commands = ["pipelight run zktls-witness"] },
  { name = "eval", commands = ["pipelight run eval-all-agents"] },
  { name = "deploy", commands = ["./deploy.sh"] },
]
